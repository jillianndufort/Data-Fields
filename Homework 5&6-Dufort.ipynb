{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "  span.ecb { background: yellow; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type=\"text/css\">\n",
    "  span.ecb { background: yellow; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATMO 5331 - Homework 5 & 6 - Fall 2019\n",
    "## Due Tue 23 Nov, 2021, 11:59 pm.\n",
    "## *Worth two assignments*\n",
    "\n",
    "When doing this homework, remember that you have two jobs:\n",
    "1. Make it work.\n",
    "2. Clean it up so that I can understand what you've done. If you think I might not understand, document it with a comment or a function docstring.\n",
    "\n",
    "You should present your work with a clear logical progression. If that seems like a hassle, remember that in doing so you are practicing skills that are expected in your thesis and journal publications.\n",
    "\n",
    "You may work alone or in pairs. I will not be adjudicating relative level of effort in group work, so you are responsible for ensuring that you and your partner contribute equally.\n",
    "\n",
    "<span class=\"ecb\">Comments by ECB</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Copy in your setup from HW3, so that you have the radar data, radar locations, and analysis grid available. Use only the tangent plane cartesian system part, and you don't need to include the plots. Take the time to clean up your original code so that it's the minimally necessary set of variables and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from coordinateSystems import TangentPlaneCartesianSystem\n",
    "from coordinateSystems import RadarCoordinateSystem\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# d = xr.open_dataset('../../../cfrad.20080604_002217_000_SPOL_v36_SUR.nc')\n",
    "d = xr.open_dataset('C:/Users/jdufo/Downloads/Ka2140614021408.RAWPXA9.nc')\n",
    "\n",
    "#print(d)\n",
    "\n",
    "# Print the first ten elevation angles.\n",
    "#print(d.variables['elevation'][0:10])\n",
    "\n",
    "# Shortcut to variable access\n",
    "#print(d.elevation[0:10])\n",
    "\n",
    "scan_idx = 1\n",
    "\n",
    "#Define the start and the end of the scan index\n",
    "idx_start = d.sweep_start_ray_index[scan_idx]\n",
    "idx_end = d.sweep_end_ray_index[scan_idx]\n",
    "\n",
    "#Create a definition that reads in the downloaded radar data and the scan index. Three arrays are created for the range, \n",
    "#azimuth, and elevation using the scan indexing\n",
    "def coords(data, scan):  \n",
    "    '''Reads in the radar data and the scan index and places them into an array\n",
    "    Return:\n",
    "    r_1d = array of the range\n",
    "    az_1d = array of the azimuth\n",
    "    el_1d = array of the elevation\n",
    "    '''\n",
    "    r_1d = np.array(data.range)\n",
    "    az_1d = np.array(data.azimuth[idx_start.values:idx_end.values+1])\n",
    "    el_1d = np.array(data.elevation[idx_start.values:idx_end.values+1])\n",
    "    \n",
    "    return r_1d, az_1d, el_1d\n",
    "\n",
    "#Call the coords definition using dataset (d) and scan index (scan_idx)\n",
    "r, az, el = coords(d, scan_idx)\n",
    "\n",
    "#Create a definition that creates three 2-d arrays for the range, azimuth, and elevation\n",
    "def coords_2d(r, az, el):\n",
    "    '''Reads in the range, azimuth, and elevation 1-d arrays from the coords definition and creates\n",
    "       a 2-d array using meshgrids\n",
    "       \n",
    "       Returns:\n",
    "       r_2d = 2-d array of the range\n",
    "       az_2d = 2-d array of the azimuth\n",
    "       el_2d = 2-d array of the elevation\n",
    "       \n",
    "    '''\n",
    "    r_2d, az_2d = np.meshgrid(r, az)\n",
    "    r_2d, el_2d = np.meshgrid(r, el)\n",
    "  \n",
    "    return r_2d, az_2d, el_2d\n",
    "\n",
    "#Call the coords_2d definition using the range, azimuth, and elevation 1-d arrays from the first definition to \n",
    "# create the three 2-d arrays\n",
    "range_2d, azimuth_2d, elevation_2d = coords_2d(r, az, el)\n",
    "\n",
    "#Using the RadarCoordinateSystem definition from the coordinateSystems.py file, use the latitude, longitude, and altitude values\n",
    "#from the radar dataset and then transform the 2-d arrays\n",
    "radar = RadarCoordinateSystem(d.latitude, d.longitude, d.altitude)\n",
    "X, Y, Z = radar.toECEF(range_2d, azimuth_2d, elevation_2d)\n",
    "\n",
    "##Using the TangentPlaneCartesianSystem definition from the coordinateSystems.py file, use the latitude, longitude, and altitude values\n",
    "#from the radar dataset and then transform the 2-d arrays\n",
    "tan = TangentPlaneCartesianSystem(d.latitude, d.longitude, d.altitude)\n",
    "\n",
    "X1, Y1, Z1 = tan.fromECEF(X, Y, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Configuration of the weighting scheme requires that we know the typical data spacing. Following TD2000, define the data spacing as the distance betweent two radar gates at the maximum range. Use the point in your analysis grid that is farthest from the radar, and then find the maximum spacing in elevation angle at this range. Finally, calculate the difference in linear units between the two radar gates you idenified.\n",
    "\n",
    "Please provide an answer to these two questions:\n",
    "- What is the maximum distance from the radar in the objective analysis domain\n",
    "- What is the spacing between two adjacent data points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Below, the function `oban` (for \"objective analysis\") mimics the call signature of the MetPy `interpolate_to_points` function. Its principal difference is the `weight_func` argument, which takes a function instead of a string describing an interpolation method. Since `weight_func` is passed only the distances, it is necessary to use `partial` to pre-fill the function with any other arguments needed to configure the weight function. The `sample_weights` function below shows how this works.\n",
    "\n",
    "For this question, your jobs are as follows.\n",
    "\n",
    "**a.**  Specify a cutoff radius. Based on the last homework assignment, what is a good distance to use as a multiple of the data spacing? Make sure to adjust your set of input data points to cover the distance from edge of the analysis grid.\n",
    "\n",
    "**b.**  Implement a `barnes` function and then use it with `oban` to calculate an analysis on the target grid. \n",
    "\n",
    "**c.**  Calculate a Barnes analysis using MetPy, as in the last assignment, and find the difference (yours - MetPy). They probably won't be the same, even for a sane configuration of parameters; that's ok.\n",
    "\n",
    "**d.**  Plot the original data, the two analyses, and the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "from functools import partial\n",
    "\n",
    "def oban(points, values, xi, weight_func, search_radius):\n",
    "    \"\"\"\n",
    "    points: N,2 data point locations\n",
    "    values: N data values\n",
    "    xi: M,2 analysis locations\n",
    "    weight_func is a function that accepts a single argument r that is the\n",
    "        distance between the analysis location and all points within search_radius\n",
    "        of the analysis location.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all points in the vicinity of each analysis location in xi\n",
    "    tree = cKDTree(points)\n",
    "    query = tree.query_ball_point(xi, search_radius)\n",
    "    \n",
    "    analysis = np.zeros(xi.shape[0])\n",
    "    \n",
    "    # This is linear (times the typical neighborhood size) in the number of analysis points\n",
    "    for i, (analysis_point, neighbors) in enumerate(zip(xi, query)):\n",
    "        data = values[neighbors]\n",
    "        data_locations = points[neighbors,:]\n",
    "        # use data, data_locations, analysis_point, and weight_func to fill in the rest of the analysis\n",
    "        analysis[i] = None # your code here\n",
    "    return analysis\n",
    "\n",
    "def sample_weights(r, value=None):\n",
    "    return np.zeros_like(r) + value\n",
    "\n",
    "my_weight_func = partial(sample_weights, value=3.0)\n",
    "my_test_ranges = np.arange(10.0)\n",
    "my_test_weights = my_weight_func(my_test_ranges)\n",
    "print(my_test_weights) # oban will call my_weight_func like so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Let's say we want to use another filter from Harris (1978) for our continuous data. Those functions are specified for discrete data, with a hard cutoff after $N$ samples. It would be logical to cut off all our analyses after the same cutoff radius for all data, so that our understanding of the filter function sidelobe behavior from discrete theory can be applied to continous data in an even-handed way.\n",
    "\n",
    "So, let's repeat the previous question, but now use Rectangular, Blackman-Harris and the difference (BH-Rect). You will need to use Harris (1978) for the mathematical formulation of the windows, as defined below.\n",
    "\n",
    "**a.**  Implement a `rect` function and then use it with `oban` to calculate an analysis on the target grid.\n",
    "\n",
    "**b.**  Implement a `blackman_harris` function and then use it with `oban` to calculate an analysis on the target grid. Use the minimum 4-term Blackman-Harris formulation as in the `scipy.signal.blackmanharris` docs whose coefficients are the -92 dB 4-term window in the table on p. 65 of Harris.\n",
    "\n",
    "**c.**  Include in this notebook, using a Markdown cell and the $\\LaTeX$ functionality, a narrated derivation that shows how you converted the discrete, non-dimensional formulation of the Blackman-Harris weight function to a continuous, dimensional form.\n",
    "\n",
    "**d.**  Make a plot of the weight functions as a function of distance from zero to your cutoff radius.\n",
    "\n",
    "**e.**  Find the difference in the weights (`rect` - `blackman_harris`).\n",
    "\n",
    "**f.**  Plot the original data, the two analyses, and the difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Let's compare the different filters.\n",
    "\n",
    "**a.** Plot the original data, and using the Barnes filter as a common point of comparison, plot the difference with the other three analyses you calculated.\n",
    "\n",
    "**b.** Which weighting scheme preserves the greatest detail in fine-scale structure? Illustrate this by discussing a local minimum and a local maximum in the original data vs. the difference fields. Does it make sense in terms of the theoretical response functions we calcualted in the previous homework?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
